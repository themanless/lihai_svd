\documentclass[format=acmsmall, review=false, screen=true]{acmart}



\usepackage{booktabs} % For formal tables

\usepackage{graphicx}
\usepackage{listings}


\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}


% Metadata Information
\acmJournal{TOMS}
\acmVolume{9}
\acmNumber{4}
\acmArticle{39}
\acmYear{2018}
\acmMonth{9}
\copyrightyear{2018}
%\acmArticleSeq{9}

% Copyright
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{0000001.0000001}

% Paper history
\received{May 2018}
\received[revised]{July 2018}
\received[accepted]{July 2018}


% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title[A Library for Low-tubal-rank Tensor Computations on GPUs]{cuTensor3D: Efficient Primitives for Third-order Low-tubal-rank Tensor Computations on GPUs}

\author{Tao Zhang}
\orcid{0000-0002-8440-9924}
\affiliation{%
  \institution{School of Computer Engineering and Science, Shanghai University}
  \streetaddress{333 Nanchen Road, BaoShan District}
  \city{Shanghai}
  \postcode{200444}
  \country{China}}
\email{taozhang@shu.edu.cn}
\author{Da Xu}
\affiliation{%
  \institution{School of Computer Engineering and Science, Shanghai University}
  \streetaddress{333 Nanchen Road, BaoShan District}
  \city{Shanghai}
  \postcode{200444}
  \country{China}}
\email{mr$\_$tab@shu.edu.cn}
\author{Xiao-Yang Liu}
\affiliation{%
 \institution{Department of Electrical Engineering, Columbia University}
 \streetaddress{streetaddress}
 \city{city}
 \state{state}
 \country{USA}}
\email{xl2427@columbia.edu}


\begin{abstract}

The low-tubal-rank tensors are widely used to model real-world multidimensional data in various applications. The alternating minimization algorithm Tubal-Alt-Min achieves the state-of-the-art performance for the low-tubal-rank tensor completion problem. However, the running time of Tubal-Alt-Min increases exponentially with the dimension of tensors, thus not very practical for medium- or large-scale tensors. In this paper, we address this problem by exploiting massively parallel GPUs. We design, implement and optimize the Tubal-Alt-Min algorithm on a GPU and evaluate the performance in terms of running time, recovery error, and convergence speed. Experiment results show that the GPU algorithm is as accurate as the CPU counterpart with 63.69x speedup on average for all tensor sizes and up to 108.88x for large  tensors. We further encapsulate the GPU algorithm into an open-source library, called \textit{cuTensor}, and demonstrate its application in video data recovery. The cuTensor library can be easily applied on many other applications of tensor completion.


\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002950.10003705</concept_id>
<concept_desc>Mathematics of computing~Mathematical software</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Mathematics of computing~Mathematical software}

%
% End generated code
%


\keywords{Tensor, Low-tubal-rank, GPU, Library}




\maketitle

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{T. Zhang et al.}


%\linenumbers

\section{Introduction}
With the rapid development and widespread applications of modern sensors and network technologies, people often need to process and analyze data with larger scale, higher dimensions and more complex structures \cite{baraniuk2011more}. These real-world data can be naturally expressed as matrices, higher-order tensors and tensor networks \cite{cichocki2015tensor}. Data loss is ubiquitous in the transmission of wireless sensor networks in the Internet of Things (IoT), which results to an incomplete
sensory dataset. However, a lot of basic scientific works, e.g. nature discovery and agriculture information surveillance, heavily rely on the accurate and complete sensory dataset to know the full view of the objects. It is significant to study how to increase the completeness of a dataset in the case of inevitable data loss. In addition, in the paradigm of edge computing, where the computing and network bandwidth resource are limited, data recovery from partial measurements
\cite{candes2006robust} has attracted tremendous interests in recent years. The down-sampled data is modeled as sparse or low-rank matrices or tensors and the recovery is done by exploiting its redundance property. Typical applications include big data analysis with missing items \cite{little2014statistical}, machine learning \cite{signoretto2014learning}, data privacy \cite{kong2015privacy}, localization \cite{liu2016adaptive} \cite{rallapalli2010exploiting}, computer vision \cite{chen2004recovering} \cite{liu2013tensor},  recommendation systems \cite{koren2009matrix}, Internet of things \cite{kong2013data} \cite{liu2015cdc} \cite{liu2017ls} and system identification \cite{liu2009interior}.



The diverse applications motivate the developments of compressive sensing in vector \cite{candes2006robust} \cite{candes2006near}, matrix \cite{candes2009exact} \cite{candes2010power} and tensor \cite{kolda2009tensor} \cite{kilmer2011factorization} \cite{kilmer2013third} models. Compressive sensing advocates replacing the $\ell_0$-norm with $\ell_1$-norm. The previously proposed iterative algorithm, called Tubal-Alt-Min \cite{XiaoYang2016Low},  is based on the alternating minimization approach for low-tubal-rank tensor completion. Tubal-Alt-Min outperforms the tensor-nuclear norm minimization method based on Alternating Direction Method of Multipliers (TNN-ADMM) \cite{zhang2017exact} in terms of recovery error, running time and convergence speed. However, Tubal-Alt-Min is computationally-intensive and its running time increases exponentially for medium- or large-scale tensors.

To address this problem, we utilize Graphics Processing Units (GPUs) to accelerate the Tubal-Alt-Min algorithm. Because of massive hardware parallelism and high memory bandwidth, GPUs have been widely used in diverse applications including machine learning \cite{cui2016geeps} \cite{brito2017detecting} \cite{campos2017scaling}, graph processing \cite{shi2018frog} \cite{zhong2017optimizing} \cite{pan2017multi}, big data analytic \cite{gutierrez2017smote} \cite{rathore2017real}, image processing \cite{devadithya2017gpu}, and fluid dynamics \cite {verma2017advanced}. In order to reap the power of GPUs, the algorithmic steps need to be mapped delicately onto the architecture of GPUs, especially the thread and memory hierarchy.  In this work, we design, implement and optimize the Tubal-Alt-Min algorithm on an NVIDIA Tesla M40 GPU with CUDA (Compute Unified Device Architecture) and evaluate it in terms of running time, recovery error and convergence speed. Experiment results show that the GPU algorithm achieves similar relative error and convergence speed as the CPU counterpart. Moreover, the GPU algorithm outperforms the CPU counterpart on all tensor sizes with 63.69x speedup on average and up to 108.88x speedup on large tensor size such as $1000 \times 1000 \times 20$. We encapsulate this GPU algorithm into an open-source library called ``cuTensor" (CUDA Tensor) so it can be applied in various applications. The cuTensor library is available at:

 http://www.findai.net/downloads.html.

Our contributions are summarized as follows. First, we analyze the steps of tensor completion using the Tubal-Alt-Min algorithm and discuss how to map them onto the GPU architecture. Second, we design, implement and optimize the Tubal-Alt-Min algorithm on an NVIDIA Tesla M40 GPU. Third, we evaluate and compare the GPU algorithm and the CPU algorithm in terms of running time, relative error and convergence speed with both synthetic data and real-world video data. Fourth, we encapsulate the GPU algorithm into an open-source library such that it can be applied in diverse applications.

The remainder of the paper is organized as follows. In Section II, we discuss the related works. Section III presents the low-tubal-rank tensor model and the Tubal-Alt-Min algorithm. Section IV describes the architecture of the NVIDIA Tesla M40 GPU, the CUDA computation and programming model, and the design, implementation and optimizations of the Tubal-Alt-Min algorithm on the GPU. In Section V, we evaluate the GPU Tubal-Alt-Min algorithm with both synthetic data and real-world video data. The conclusions are given in Section VI.



\section{Background}
\subsection{Third-order Low-tubal-rank Tensor model}

Following the pervious paper \cite{XiaoYang2016Low} \cite{liu2017fourth}, we will focus on
third-order tensors denoted by uppercase calligraphic letter,
$\mathcal{X} \in \mathbb{R}^{m \times n \times k}$. Lowercase boldface
letters $\mathbf{x}$, $\mathbf{y} \in \mathbb{R}^n$ denote vectors, and
uppercase letters $\mathbf{X}$, $\mathbf{Y} \in \mathbb{R}^{m \times n}$ denote matrices.

Let $\mathbf{X}^{\dagger}$ denote the transpose of matrix. We use
$i, j, \ell$ to index the first, second and third dimension of a tensor,
and $s, t$ for temporary index.
$[n]$ denotes the set $\{1, 2, \cdots, n\}$. Usually, $i\in [n]$, $j\in [m]$, $\ell\in [k]$ unless otherwise specified.
 For tensor $\mathcal {T} \in \mathbb{R}^{m \times n
\times k}$, the $(i, j, \ell)$-th entry is $\mathcal{T}(i, j, \ell)$, or
concisely represented as $\mathcal{T}_{ij\ell}$.


%%f-norm of tensors
The $\ell_2$-norm of a vector is defined as $\|\mathbf {a} \|_2 = \sqrt{\sum _{i = 1} \mathbf {a}^2_i}$, while the Frobenius norm of a matrix $\mathbf{X}$ is $\|\mathbf{X}\|_F = \sqrt{\sum ^m_{i = 1} \sum ^n_{j = 1} X^2_{ij} }$ and a tensor is $\|\mathcal{T}\|_F = \sqrt{\sum ^m_{i = 1} \sum ^n_{j = 1} \sum ^k_{\ell = 1} \mathcal{T}^2_{ij\ell}}$.


\paragraph{Tubes, fibers and slices of a tensor} A tube (also called
a fiber) is a 1-D section defined by fixing all indices but one, while
a slice is a 2-D section defined by fixing all but two indices. We use
$\mathcal{T}(:, j, \ell)$, $\mathcal{T}(i, :, \ell)$, $\mathcal{T}(i, j, :)$ to
denote the ($i$, $j$)-th model-1, model-2, model-3 tubes, which are
vectors, and $\mathcal{T}(:, :, \ell)$, $\mathcal{T}(:, j, :)$, $\mathcal{T}(i, :, :)$ to denote the frontal, lateral, horizontal slices, which are matrices. 

%%follow the Third order tensor paper begin{


\subsection{Tensor Operations and Key Processes}
In order to discuss multiplication between two tensors and to understand the basics of the algorithms we consider here, we first must introduce the concept of converting $\mathcal{A} \in \mathbb{R}^{m \times n \times k}$ into a block circulant matrix.

If $\mathcal{A} \in \mathbb{R}^{m \times n \times k}$ with $m \times n$ frontal slices then
$$
\text{circ}(\mathcal{A}) = 
    \begin{bmatrix}
    \mathcal{A}(:,:,1) & \mathcal{A}(:,:,n) & \mathcal{A}(:,:,n-1) & \ldots & \mathcal{A}(:,:,2) \\
    \mathcal{A}(:,:,2) & \mathcal{A}(:,:,1) & \mathcal{A}(:,:,n) & \ldots & \mathcal{A}(:,:,3) \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \mathcal{A}(:,:,n) & \mathcal{A}(:,:,n-1) & \ldots & \mathcal{A}(:,:,2) & \mathcal{A}(:,:,1)
    \end{bmatrix} ,
$$
%% cir(A)
is a block circulant matrix of size $mk \times nk$.

We anchor the $\text{Matvec}$ command to the frontal slices of the tensor.
$\text{MatVec}(\mathcal{A})$ takes an $m \times n \times k$ tensor and returns a block $mk \times n$ matrix, whereas the fold command undoes this operation
$$
    \text{MatVec}(A)= 
    \begin{bmatrix}
    \mathcal{A}(:,:,1) \\
    \mathcal{A}(:,:,2) \\
    \vdots \\
    \mathcal{A}(:,:,n)\\
    \end{bmatrix}
    ,  \text{fold}(\text{MatVec}(\mathcal{A})) = \mathcal{A}
$$


%Matvec
when we are dealing with matrices, the $\text{vec}$ command unwraps the matrix
into a vector by column stacking, so that in Matlab notation $\text{vec}(A) \equiv A(:)$.

\paragraph{t-product} Let $\mathcal{A}$ be $m \times p \times k$ and $\mathcal{B}$ be $p \times n \times k$. Then the t-product $\mathcal{A} \ast \mathcal{B}$ is the $m \times n \times k$ tensor 
\begin{equation}
    \mathcal{A} \ast \mathcal{B} = \text{fold}(\text{circ}(\mathcal{A}) \cdot
    \text{MatVec}(\mathcal{B})).
\end{equation}

%%extrat from Third order tensor pape
\paragraph{Tensor transpose and frequency domain representation}
$\mathcal{T}^{\dagger}$ is obtained by transposing each of the frontal slices and then reversing the order of transposed frontal slices 2 through
$k$, i.e., $\mathcal{T}^{\dagger}(:,:,1) = (\mathcal{T}(:,:,1))^{\dagger}$, and for $2 \leq \ell \leq k$,
$\mathcal{T}^{\dagger}(:, :, \ell) = \mathcal{T}^{\dagger(\ell)} = (\mathcal{T}(:, :, k +2 -\ell))^{\dagger}$ (the transpose of
matrix $\mathcal{T}(:, :, k + 2 - \ell))$. For reasons to become clear soon, we define a tensor $\widetilde{\mathcal{T}}$,
which is obtained by taking the Fourier transform along the third dimension of $\mathcal{T}$, i.e., $\widetilde{\mathcal{T}}(i, j, :) = \mathrm{fft}(\mathcal{T}(i, j, :))$.

%% update in 9/12
\paragraph{Identity tensor} The Identity tensor $\mathcal{I} \in
\mathbb{R}^{n\times n \times k} $ is a tensor whose first frontal
slice $\mathcal{I}(:, :, 1)$ is the $n \times n $ identity matrix and
all other frontal slices are zero.
%% update in 9/12

\paragraph{Inner Products} Let $\mathcal{X}, \mathcal{Y}, \mathcal{Z}$ $\in
\mathbb{K}^{m \times 1 \times n}$ and let $\mathbf{a} \in \mathbb{K}^{1 \times 1
\times n}$. Then $\langle \mathcal{X}, \mathcal{Y} \rangle := \mathcal{X}^T \ast \mathcal{Y}$ satisfies the following
\begin{enumerate}
    \item $\langle \mathcal{X}$, $\mathcal{Y} + \mathcal{Z} \rangle$ = $\langle$ $ \mathcal{X}, \mathcal{Y} \rangle$ + $\langle \mathcal{X},\mathcal{Z} \rangle$
    \item $\langle$ $\mathcal{X},\mathcal{Y} \ast \mathbf{a} \rangle =
        (\mathcal{X}^{\dagger} \ast \mathcal{Y}) = \mathbf{a} \ast
        (\mathcal{X}^{\dagger} \ast \mathcal{Y}) = \mathbf{a} \ast \langle$ $\mathcal{X},\mathcal{Y} \rangle$
    \item $\langle$ $\mathcal{X},\mathcal{Y} \rangle$ = $\langle
        \mathcal{Y},\mathcal{X} \rangle^{\dagger}$
\end{enumerate}
\paragraph{Orthogonal tensor} A tensor $\mathcal{Q} \in \mathbb{R}^{n
\times n \times k}$ is Orthogonal if it satisfies
$\mathcal{Q}^{\dagger} * \mathcal{Q} = \mathcal{Q} *
\mathcal{Q}^{\dagger} = \mathcal{I}$.
%% update in 9/12
\paragraph{Inverse} The inverse of a tensor $\mathcal{T} \in
\mathbb{R}^{n \times n \times k} $ is written as $\mathcal{T}^{-1} \in
\mathbb{R}^{n \times n \times k} $ and satisfies $\mathcal{T}^{-1}
*\mathcal{T} = \mathcal{T} * \mathcal{T}^{-1} = \mathcal{I}.$
%% update in 9/12
\paragraph{Block diagonal form of third-order tensor} Let
$\overline{\mathcal{A}}$ denote the block-diagonal matrix of the tensor
$\mathcal{A}$ in the Fourier domain, i.e.,
\[
    \overline{\mathcal{A}} \doteq \mathrm{blkdiag}(\widetilde{\mathcal{A}}) \doteq
\begin{bmatrix}
    \widetilde{\mathcal{A}}(:, :, 1) & & &\\
    & \widetilde{\mathcal{A}}(:, :, 2) & &\\
    & & \dots &\\
    & & & \widetilde{\mathcal{A}}(:, :, k)
\end{bmatrix}
\in \mathbb{C}^{mk \times nk}
\]
It is easy to verify that the block diagonal matrix of
$\mathcal{A}^\dagger$ is equal to the transpose of the block diagonal
matrix of
$\mathcal{A}: \overline{\mathcal{A}^{\dagger}} = {\overline{\mathcal{A}}}^{\dagger}. $
%% update in 9/12
\paragraph{F-diagonal tensor} A tensor is called f-diagonal if each
frontal slice of the tensor is a diagonal matrix, i.e., $\Theta(i, j, \ell)
= 0$ for $i \neq j$, $\forall \ell$.
Using all these definitions and constructs, the
following singular value decompositions, referred to as t-SVD was
obtained for dimensionality reduction of third order data.
\paragraph{T-SVD}For $\mathcal{T} \in \mathbb{R}^{m \times n \times
k}$, the t-SVD of $\mathcal{T}$ is given by $\mathcal{T} =
\mathcal{U} * \Theta * \mathcal{V}^{\dagger}$, where $\mathcal{U}$ and
$\mathcal{V}$ are Orthogonal tensors of sizes $m \times m \times k$
and $n \times n \times k$, respectively. $\Theta$ is a f-diagonal
tensor of size $m \times n \times k$ and the tubes are called the
eigentubes of $\mathcal{T}$.
%% update in 9/12
%Algorithm to be updated
\paragraph{Tensor tubal-rank} The tensor tubal-rank of a third-order
tensor $\mathcal{T} $ is the number of non-zero tubes of $\Theta$ in
the t-SVD, denoted as $r$.
%% update in 11/1
Suppose $\mathcal{T}$ has tubal-rank $r$, then the
reduced t-SVD of $\mathcal{T}$ is given by $\mathcal{T} = \mathcal{U}
 * \Theta * \mathcal{V}^{\dagger}$, where $\mathcal{U} \in
 \mathbb{R}^{m \times r \times k}$ and $\mathcal{V} \in
 \mathbb{R}^{n \times r \times k}$ satisfying $\mathcal{U}^{\dagger} *
 \mathcal{U} = \mathcal{I}$, $\mathcal{V}^{\dagger} * \mathcal{V} =
 \mathcal{I}$, and $\Theta$ is a f-diagonal tensor of size $r \times r
 \times k$. This reduced version of t-SVD will be used throughout the
 paper unless otherwise noted.
%% update in 11/12

\subsection{Tensor Completion Application}

\paragraph{Best rank-r approximation} Let the t-SVD of $\mathcal{T}
\in \mathbb{R}^{m \times n \times k}$ be $\mathcal{T} = \mathcal{U} *
 \Theta * \mathcal{V}^{\dagger}$ and positive integer $r$, define
$\mathcal{T}_{r} = \sum_{s = 1}^r \mathcal{U}(:, s, :) *
 \Theta(s, s, :) \ast \mathcal{V}^{\dagger}(:, s, :)$, then
\[
\mathcal{T}_r =
\arg \underset{\overline{ \mathcal{T}} \in \mathbb{T}} \min \ \| \mathcal{T} - \overline{
\mathcal{T}} \|_F^2 ,
\]
where $\mathbb{T} = \{\mathcal{X} \ast
\mathcal{Y}^{\dagger} | \mathcal{X} \in \mathbb{R}^{m \times r \times k},
\mathcal{Y} \in \mathbb{R}^{n \times r \times k}\}$.

\paragraph{The Alternating Minimization Algorithm}

The main idea of the algorithm is very similar to that of the alternating minimization for matrix completion  and tries to iteratively estimate two low-tubal-rank factors $\mathcal{X} \in \mathbb{R}^{m \times n \times k}$ and $\mathcal{Y} \in \mathbb{R}^{r \times n \times k}$ each of tubal rank r, such that
\[
    \mathcal{T} = \mathcal{X} * \mathcal{Y}. \notag
\]

For simplicity, denote
$\mathcal{T}_{\Omega} = \mathcal{P}_{\Omega}(\mathcal{T})$ and let
$\mathcal{P}_{\Omega}$ be the sampling tensor with ones at places where the tensor is
sampled and zero otherwise. Then we have $\mathcal{T}_{\Omega} = \mathcal{P}_{\Omega}\odot \mathcal{T}$ where $\odot$ is the element-wise multiplication of same
size arrays. For the $(i, j)$-th tube, $\mathcal{T}_{ij}=\mathcal{P}_{ij}\odot \mathcal{T}_{ij}$. According to the convolution theorem, we can transform the least squares minimization to the following frequency domain version:
\[
    \widetilde{\mathcal{Y}} = \mathop{ \arg \min}_{\widetilde{\mathcal{Y}} \in \mathbb{R}^{r \times n \times k}} \| \widetilde{\mathcal{T}}_{\Omega} -  \widetilde{\mathcal{P}}_{\Omega} \cdot \otimes (\widetilde{\mathcal{X}} \cdot \S  \widetilde{\mathcal{Y}}) \| ^2_F,
\]
where $\cdot \ \otimes$ denotes tube-wise circular convolution,
and $\cdot \ \S $ denotes front-slice-wise matrix multiplication. Note that
the above problem can be split into n separate subproblem:
\[
    \widetilde{\mathcal{Y}}(:, j, :) = \mathop{\arg \min}_{\widetilde{\mathcal{Y}} \in \mathbb{R}^{r \times 1 \times k}} \| \widetilde{\mathcal{T}}_{\Omega}(:, j, :) -  \widetilde{\mathcal{P}}_{\Omega}(:, j, :) \cdot \otimes (\widetilde{\mathcal{X}} \cdot \S \widetilde{\mathcal{Y}}(:, j, :)) \|^2_F,
\]
where $\mathcal{X}^T$ denotes the tube-wise transpose (the transpose of a matrix of vectors). To solve this performs the following steps. A lateral slice, $\widetilde{\mathcal{T}}_{\Omega}(:, j, :)$ of
size $m \times 1 \times k$, is squeezed into a vector $b$ of size
$mk \times 1$ in the following way:
\[
\mathbf{x} = [\mathrm{squeeze}(\widetilde{\mathcal{Y}}_{\Omega}(1, j, :)); \mathrm{squeeze}(\widetilde{\mathcal{Y}}_{\Omega}(2, j, :)); \dots; \mathrm{squeeze}(\widetilde{\mathcal{Y}}_{\Omega}(m, j, :))];
\]
$\widetilde{\mathcal{X}}$ is transformed into a block diagonal matrix $\mathbf{A}_1$
of size $mk \times rk$ like so,
\[
\mathbf{A}_1 =
\begin{bmatrix}
    \widetilde{X}(:, :, 1) & & \\
    & \widetilde{X}(:, :, 2) & \\
    & & & \\
    & & \widetilde{X}(:, :, k)
\end{bmatrix} .
\]
The $j$-th lateral slice $\widetilde{\mathcal{P}}_{\Omega}(:, j, :)$ is transformed into a tensor $\mathcal{A}_2$ of size $k \times k \times m$ first,
and then into a matrix $A_3$ of size $mk \times mk$
\[
\mathcal{A}_2(:, :, \ell) = \mathrm{circ}(\widetilde{\mathcal{P}}_{\Omega}(:, j, :)), \ell \in [m],
\]
where circ(v) is a square circulate matrix formed from a vector
$\mathbf{v}$, and the $i$-th column is a circularly shifted version
of vector $\mathbf v$, whose elements are a circularly shifted down by amount $i$.

\begin{figure}[t]
    \centering
    \includegraphics[width=3in]{GPU_algorithm.pdf}
    \caption{Steps of tensor completion with the Tubal-Alt-Min algorithm.}
    \label{pic_gpu_algorithm}
\end{figure}

\[
\mathbf{A}_3 =
\begin{bmatrix}
    \mathrm{diag}(\mathcal{A}_2(1, 1, :)) & \mathrm{diag}(\mathcal{A}_2(1, 2, :)) & \ldots & \mathrm{diag}(\mathcal{A}_2(1, k, :)) \\
    \mathrm{diag}(\mathcal{A}_2(2, 1, :)) & \mathrm{diag}(\mathcal{A}_2(2, 2, :)) & \ldots & \vdots \\
    \vdots & \vdots & \vdots & \vdots & \\
    \mathrm{diag}(\mathcal{A}_2(k, 1, :)) & \ldots &\dots & \mathrm{diag}(\mathcal{A}_2(k, 1, :))
\end{bmatrix} .
\]
where the operator diag() transforms a tube into a diagonal matrix by putting the elements in the diagonal. \par
The estimation of the j-th lateral slice is transformed into the following standard least squares minimization problem:
\[
\widetilde{\mathbf{x}} = \mathop{\arg \min}_{\mathbf{x} \in
\mathbb{R}^{rk \times 1}} \| \mathbf{b} - \mathbf{A}_3\mathbf{A}_1 \mathbf{x} \|^2_F.
\]

\section{Related Work}
In this section, we will review related works from four aspects including tensor models, tensor completion algorithms for low-tubal-rank tensor model, tensor computation on GPUs and software libraries on GPUs.

In existing models \cite{kolda2009tensor} \cite{papalexakis2016tensors} \cite{oseledets2011tensor} \cite{cichocki2015tensor}, tensors are treated as multidimensional arrays of real values such that algebraic operation such as matrix calculation can be performed. The corresponding tensor spaces are therefore viewed as the tensor product of vector spaces, including 1) the canonical polyadic (CP) decomposition, the Tucker decomposition, and their variants \cite{kolda2009tensor}; 2) the higher-order SVD (HOSVD) \cite{de2000multilinhttps://v2.overleaf.com/project/5aeeaf60bf83460d0d26fcd9ear}; 3) the recently proposed tensor-train decomposition \cite{oseledets2011tensor} and the tensor ring decomposition \cite{zhao2016tensor}; and 4) tensor networks \cite{cichocki2016tensor}. All these tensor models rely on either contraction products (e.g., the vector inner product and the $n$-mode product \cite{kolda2009tensor} or expansion products (e.g., the Kronecker product and the vector outer product). However, these two kinds of products change tensor order. Therefore, the corresponding tensor spaces lack the closure property, being fundamentally different from the well-studied conventional matrix space. The low-tubal-rank tensor model \cite{kilmer2011factorization} \cite{kilmer2013third} is the first trial to extend the conventional matrix space to third-order tensors \cite{braman2010third}. Under this framework, \cite{kilmer2011factorization} \cite{kilmer2013third} generalized classical algorithms in the conventional matrix space, such as SVD, QRD, normalization, the Gram-Schmidt procedure, power iteration, and Krylov subspace methods.

Recently, the low-tubal-rank tensor model has been applied in various areas, such as data completion \cite{zhang2017exact} \cite{XiaoYang2016Low}, MRI imaging \cite{semerci2014tensor} and two-dimensional dictionary learning \cite {jiang2017graph}. Work \cite{zhang2017exact} \cite{zhang2014novel} showed that given a sufficient number of observations, tensor-nuclear norm minimization results in exact recovery under random sampling if the tensors satisfy certain tensor incoherency conditions.
However, the computational cost of the TNN-ADMM algorithm in \cite{zhang2017exact} is relatively high due to two key factors: 1) Each iteration requires computation of SVD for large block diagonal matrices, and 2) The iterations are jointly done in both the time domain and frequency domain, thus involving frequent and large number of Fourier and inverse Fourier transforms. Therefore, we proposed the Tubal-Alt-Min algorithm \cite{XiaoYang2016Low} which is more accurate and
faster than the TNN-ADMM algorithm for low-tubal-rank tensor completion.

Many existing researches \cite{jing2016energy} \cite{zhang2015buddy} \cite{zhang2015efficient} \cite{zhang2014cuirre} \cite{zhang2016efficient} have demonstrated the benefit of utilizing GPUs to accelerate general purpose computing. Because of the massive parallelism and high memory bandwidth, GPUs are often used in accelerating machine learning applications \cite{chetlur2014cudnn} \cite{cui2016geeps} \cite{tsung2016high} \cite{banasiak2016statistical}.  Recently it has been applied to accelerate tensor contractions \cite{shi2016tensor} and the proposed new BLAS-like primitive called STRIDEDBATCHEDGEMM is capable of performing a wide range of tensor contractions on CPU and GPU efficiently. Sparse tensor-times-dense matrix multiply is a critical bottleneck in data analysis and mining applications and \cite{li2016optimizing} proposed an efficient primitive on CPU and GPU platforms. Solving large numbers of small linear algebra problems simultaneously is becoming increasingly important in many application areas and \cite{dongarra2017optimized} made a systematic work to optimize batched linear algebra for GPUs.

CUDA is the computing and programming model proposed by NVIDIA that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems much faster than on CPUs. CUDA itself has many powerful libraries that provide various functionality in scientific computing, such as cuBLAS \cite{CUBLAS} for linear algebra, cuFFT \cite{CUFFT} for FFT and inverse FFT, nvGraph \cite{NVGRAPH} for graph computing and so on. Researchers have proposed a number of third-party libraries for GPU computing. In \cite{zhang2014cuirre}, we proposed the cuIRRE library for improving performance of irregular applications on GPUs resulting from irregular loop structures. In addition, cuIRRE can characterize irregular applications for their irregularity, thread granularity and GPU utilization. The cuDNN library \cite{chetlur2014cudnn} was proposed which implemented a group of efficient deep learning primitives on GPUs. cuDNN can be integrated into Caffe to improve performance significantly. The STRIDEDBATCHEDGEMM primitives in \cite{shi2016tensor} have been incorporated in the cuBLAS \cite{CUBLAS} library to accelerate tensor contraction operations.

\section{Efficient Primitives for Third-order Low-tubal-rank Tensor Computations on GPUs}




\subsection{The GPU Architecture and the Computation Paradigm}
\begin{figure}[t]
    \centering
    \includegraphics[width=4.5in]{M40_architecture.pdf}
    \caption{Architecture of the M40 GPU.}
    \label{pic_m40_architecture}
\end{figure}
The architectures of NVIDIA GPUs have evolved many generations to push the performance and energy efficiency to a higher and higher level. For instance, the energy efficiency of a GTX750 Ti GPU (Maxwell architecture) is 4 times of a GTX480 GPU (Fermi architecture). The Tesla M40 GPU of Maxwell architecture was announced in late 2015, which was the first NVIDIA GPU focused on the machine learning market. It delivers 7 Tera flops single-precision performance and can have up to 24 GB of GDDR5 memory on board for training large deep learning models. Maxwell GPUs incorporate an all-new design of the streaming multiprocessors (called ``SMM" in Maxwell), which is quite different with the streaming multiprocessors in Kepler (called ``SMX"). The SMM implements a number of architectural enhancements, including improvements to control logic partitioning, workload balancing, clock-gating granularity, scheduling, number of instructions issued per clock cycle, and many other improvements. These enhancements enabled SMMs to achieve much smaller size and much higher performance than SMX. Fig. \ref{pic_m40_architecture} shows the block diagram of the M40 GPU. As shown in the figure, there are 6 Graphics Processing Clusters (GPCs) in the M40 GPU, each containing 4 SMMs. Each SMM is partitioned into four separate processing units, each containing its own instruction buffer, warp scheduler and 32 CUDA cores.  There is a 64 KB shared memory shared by all 4 processing blocks within an SMM. Besides, every two processing blocks share a texture cache and four texture filtering units. We will summarize the device specifications of the M40 GPU in the experiment section.

CUDA is the GPU computation model and programming environment introduced by NVIDIA. CUDA C is an extension to the standard C programming language. Programmers develop kernels which will be executed on the GPU device. In GPU execution, hardware threads are organized into a hierarchy of grid, blocks and warps. Every 32 threads are grouped into a warp for synchronized execution on a processing unit. Then one warp or multiple warps form a block and finally all blocks form a grid. The number of blocks and the number of threads in each block are explicitly specified by the kernel launch function. Blocks are assigned to streaming multiprocessors based on a round-robin scheme, and the warps and blocks residing in a streaming multiprocessor are referred as active warps and active blocks, respectively. More concurrent threads can not only improve parallelism, but also hide the latencies of instruction \& execution units as well. M40 GPU supports maximum 1024 threads in each block and 2048 threads in each SMM. The number of concurrent threads is also limited by hardware resources, especially on-chip shared memory and registers.


\subsection{Batched Tensor Compute Model}
It is well-known that block circulant matrices can be block diagonalized by using the Fourier transfrom. Mathematically, this means that if $F$ denotes the $n \times n$ (normalized) DFT matrix, then for $\mathcal{A} \in \mathbb{R}^{m \times n \times k}$, there exit $n$, $m \times n$ matries $\widetilde{A}^{(i)}$, possibly with complex entries, such that
$$
(F \otimes I)\mathrm{circ}(\mathcal{A})(F^{\ast} \otimes I) = \mathrm{blockdiag}(\widetilde{A}^{(1)},..., \widetilde{A}^{(n)}).
$$
But as the notation in the equation is intended to indicate, it is not necessary to form $\mathrm{circ}(\mathcal{A})$ explicitly to generate the matrices $\widetilde{\mathcal{A}}^{(i)}$. Using Matlab notation, define $\widetilde{\mathcal{A}} := \mathrm{fft}(\mathcal{A},[],3)$ as the tensor obtained by applying the FFT along each tubal-element of $\mathcal{A}$. Then $\widetilde{A}^{(i)} := \widetilde{A}(:,:,i)$. 
for example, to compute $\mathcal{C} = \mathcal{A} \ast \mathcal{B}$, we could compute $\widetilde{\mathcal{A}}$ and $\widetilde{\mathcal{B}}$, the perform matrix multiplies of individual pairs of faces of these to give the faces of $\widetilde{\mathcal{C}}$, and then $\mathcal{C} = ifft(\widetilde{\mathcal{C}}, [], 3)$.

In our low-tuble-rank tensor model, most explended operations from matrix to tensor follow the steps below:
\begin{enumerate}
    \item the tensor objects take fft along the third dimension
    \item the fourier transformed tensor objects perform the corresponding matrix operations slice by slice individually; 
    \item the fourier transformed tensor objects take ifft along the third dimension
\end{enumerate}

\begin{tabular}{|c|c|c|}
    \hline
     Tensor Operation & Basical Method & Parallel Strategy  \\
     \hline
     T-Product & cublas<t>gemm() & cublas<t>Bathedgemm() and CUDA stream \\
     \hline
     T-SVD & cusolver<t>gesvdj() & cusolver<t>Bathedgesvdj() and CUDA stream \\
     \hline
     T-QR & \textup{cusolverDn<t>geqrf()} & blockdiagonal matrices and CUDA stream \\
     \hline
     T-Inverse & cusolver<t>gesvdj() & batched CUDA kernels and CUDA stream \\
     \hline
     T-Norm & cublas<t>dot() & batched CUDA kernels and CUDA stream \\
     \hline
\end{tabular}

To implement these operations in GPU, we propose the Batched Tensor Compute model. In the first step, named as t-FFT, we perform FFT on the tensor objects along the third dimension, which will be detailed in the following section. In the second step, named as Batched Compute, as the data in different slice is independed, We use corresponding parallel strategies to parallelize batch matrix operations. Finally, we perform iFFT on the results in frequency domain to get the final results.

%%the goodness of our model
Our model provide a gerneral strategy to optimaize tensor computation. In the Batched Compute step, we can adopt different optimal optimization strategies for different data sizes and operations. For small size of data, we can utilize the corresponding batched functions provided by CUDA libraries. For example, cublasSgemmStridedBatched() for matrix-matix multiplication of a batch of matrices. For middle or large size of data, we can set multi CUDA streams to perform the operations simultaneously. We have used our proposed model and corresponding parallel strategies to achieve some major tensor operations, which will be detailed in following sections.

%%the goodness of cuda libraries
NVIDIA and other institutions provide domain-specific CUDA libraries that can be used as building blocks for more complex applications. These
libraries have been optimized by CUDA experts and designed to have high-level, highly-usable APIs with standardized data formats to facilitate their ability to plug in to existing applications
(pluggability). CUDA libraries sit on top of the CUDA runtime, providing a simple, familiar, and
domain-specifi c interface for both host applications and third-party libraries.

%%Table of functions TBD

\subsection{Tensor Data Structures and Primitives}
In CUDA libraries, matrix are stored in 1-D array. 
As the operations are performed slice by slice individually, 
 we use a 1-D array of tensor slices to represent a tensor, as following:
\[
    \mathbf{T}[m \times n \times \ell] = [\mathrm{squeeze}(\mathcal{T}(:, 1, \ell)); \mathrm{squeeze}(\mathcal{T}(:, 2, \ell)); \dots; \mathrm{squeeze}(\mathcal{T}(:, n, \ell))] ,
\]
where slice by slice, $\mathcal{T}(:, :, \ell)$ of size $ m \times n \times
1$ , is squeeze into an array of size $ mn \times 1$, and finally we get
an array of size $mnk \times 1$ for a tensor $\mathcal{T} \in
\mathbb{R}^{m \times
n \times k}$, as shown in Fig. \ref{pic_tensor_representation}.
\begin{figure}[t]
    \centering
    \includegraphics[width=4.5in]{Tensor_representation.pdf}
    \caption{Tensor representation in memory.}
    \label{pic_tensor_representation}
\end{figure}
As a result, we can access slices with a constant stride. 
\begin{lstlisting}[language=C]
T-operation(op_type opA, op_type opB,
            int m, int n, int k,
            const T* A, int lda, int loa,
            const T* B, int ldb, int lob,
            T* C, int ldc, int loc,
int batch_size)
{
    for (int p=0; p < batch_size; ++p)
        M-operation(opA, opB,
                  m, n, k,
                  A + p*loa, lda,
                  B + p*lob, ldb,
                  C + p*loc, ldc);
}
\end{lstlisting}
The list shows how to apply our proposed model with matrix-based CUDA library functions to perform the tensor operations pratically.The $\mathrm{T-operation}$ and $\mathrm{M-operation}$ denotes functions performing tensor operation and the corresponding matrix operation.
The $\mathrm{lda, ldb, ldc}$ parameters are the leading dimension of a two-dimensional array used to store the matrices and denote to the stride between columns of the matrix. We refer to the new $\mathrm{loa, lob, loc}$ parameters to denote the stride between matrices of the batch. We can expand the loop and parallel it on GPU.
\subsection{Optimization of Tensor Operations}
\subsubsection{T-FFT}
\label{SEC_Tensor_FFT}
\begin{figure}[t]
    \centering
    \includegraphics[width=4.5in]{Tensor_fft.pdf}
    \caption{Tensor fft on GPU.}
    \label{TensorFFT}
\end{figure}
  Since $\widetilde{\mathcal{T}}$ is obtained by conducting a Fourier transform along the third dimension of $\mathbf{t}$, we arrange the data of $\mathbf{T}[m \times n \times k]$ with the priority of the third tubal. Therefore we define the arranged $T$ as:
\[
 [\dots; \mathrm{squeeze}(\mathcal{T}(i, j, :)); \dots]\notag
\]

Fig. \ref{TensorFFT} shows the arranged $\mathbf{t}$ is transferred from the host memory to the GPU device memory in one step. Then we apply the CUDA API batched cuFFT() to perform the Fourier transform without looping
over all tubes and making the transform separately. This strategy allows us to better utilize the computing resources on the GPU and achieve good performance.

\subsubsection{T-product}.Following our proposed Tensor Batched Compute Model, we first apply the T-FFT described above to convert the tensors into frequency domain. In the Batched Compute step, we use different parallel strategies to accelerate the entire operations for data of different sizes. For a tensor with small first tow dimensions, whose frontal sice matrix is small, the cuBLAS provides \textup{cublas<t>gemmBatched()} to compute this problem well. For a tensor with large first tow dimenfions, whose frontal sice matrix is large, there is not enough space to perform \textup{cublas<t>gemmBatched()}. So we set the same number of streams as the batch. Then each stream perform \textup{cublas<t>gemm()} to do the multiplication between a pair of slices.
%%the list of T-product T-SVD T-QR

\subsubsection{T-SVD}
Following our proposed Tensor Batched Compute Model, we first apply the T-FFT described above to convert the tensor into frequency domain. In the Batched Compute step, we use different parallel strategies to accelerate the entire operations for data of different sizes. For a tensor with small first tow dimensions, whose frontal sice matrix is small, the cuSOLVER provides \textup{cusolver<t>gesvdjBatched()} to compute this problem well. \textup{gesvdjBatched} performs gesvdj on each matrix. It requires that all matrices are of the same size m,n no greater than 32. For a tensor with large first tow dimenfions, whose frontal sice matrix is large, \textup{gesvdjBatched} does't work.  So we set the same number of streams as the batch. Then each stream perform \textup{cusolver<t>gesvdj()} to do the SVD oen each slice.

\subsubsection{T-Inverse}
Following our proposed Tensor Batched Compute Model, we first apply the T-FFT described above to convert the tensor into frequency domain. In the Batched Compute step, we use different parallel strategies to accelerate the entire operations for data of different sizes. We use the SVD decomposition method to get the matrix inverse. The first step follows SVD decomposition in T-SVD described above. The second step takes the inverse of each element of $S$, which is data-independed. So we start a kernel for the inverse of each element. These kenels excute in parallel, which is much faster than using looping serial execution.
Then we use \textup{cublas<t>gemmBatched()} to perform matrix-matrix multiplication as T-product do. 

\begin{lstlisting}[language=C]
__global__ void(var *S)
{
    S[threadID] = 1.0 / S[threadID]
}
\end{lstlisting}

\subsubsection{T-norm}
Since the F norm of a tensor is defined as the square root of the sum of squares of all elements, we can use $\mathrm{cublas<t>dot()}$ to get the inner product of two arrays to compute the sum of squares of all elements. Mathmatically, we have $\langle \vec{\mathbf{a}}, \vec{\mathbf{a}} \rangle = \Sigma^n_{i=1}\vec{\mathbf{a}}_i$. Finally, it is not difficult to get the square root of the sum.


\begin{algorithm}
\caption{Computing the inverse of a matrix using SVD}
\begin{algorithmic}
\STATE step 1:$[U, S, V] = \mathrm{svd}(A)$

\STATE step 2:$T =S$

\STATE $T(\mathrm{find}(S \neq 0) = 1./S(\mathrm{find}(S \neq 0)$

\STATE step 3:$\mathrm{inv}(A) = V \ast T^{T} \ast U^{T}$

\end{algorithmic}
\end{algorithm}
\subsubsection{T-QR}
Following our proposed Tensor Batched Compute Model, we first apply the T-FFT described above to convert the tensor into frequency domain. In the Batched Compute step, we use different parallel strategies to accelerate the entire operations for data of different sizes. cuSOLVER provide \textup{cusolverDn<t>geqrf()} to do the QR decomposition for a single dense matrix. For a tensor with large first tow dimenfions, whose frontal sice matrix is large, \textup{gesvdjBatched} does't work.  So we set the same number of streams as the batch. Then each stream perform \textup{cusolver<t>gesvdj()} to do the SVD on each slice. However,
cuSOLVER doesn't provide a  batched version of \textup{cusolverDn<t>geqrf()} as  \textup{cusolver<t>gesvdj()} for T-SVD for small size of data.

%the equation of blkdiag of qr
Let $\mathcal{Q}(:,:,i)$ and $\mathcal{R}(:,:,i)$ denote the Q and R part of the QR decomposition $[\mathcal{Q}(:,:,i) ,\mathcal{R}(:,:,I)] = qr(\mathcal{A}(:,:,i))$. Mathematically, we have   $[blockdiag(\mathcal{Q}(:,:,1),...,\mathcal{Q}(:,:,i)), blockdiag(\mathcal{R}(:,:,1),...,\mathcal{R}(:,:,i))] = qr(blockdiag(\mathcal{A}(:,:,1),...,\mathcal{A}(:,:,i))$. 

So we do $\mathrm{blockdiag}(\widetilde{A}^{(1)},..., \widetilde{A}^{(n)})$, then we do QR decomposition on this blockdiaged matrix equivalent to decompositions on a bath slices. Notice that this block diagonal matrix has many zero elements, which means the matrix is sparse. Therefore, we use the CSR format to store this matrix, which not only saves storage space, but also allows us to perform computing using the cuSPARSE method provided by the CUDA library. 

\subsection{Tensor Completion Application}
By properly mapping the computational steps onto the hardware threads running simultaneously on the GPU, we are able to gain considerable speedups. In this section, we will present the GPU architecture, the representation of tensor data, the efficient design and implementation of FFT, matrix computation, least squares minimization, t-svd on the GPU architecture, and optimizations.

Fig. \ref{pic_gpu_algorithm} illustrates the computational steps of the tensor completion process which consists of three stages: the initializing stage, the Tubal-Alt-Min algorithm stage and the postprocessing stage. The initializing stage performs sampling, FFT and t-product to prepare the incomplete tensor data for testing. The optional sampling operation is used if we need to sample a complete tensor data, for example sampling a real video to get a sampled video as the input for tensor completion. In the Tubal-Alt-Min algorithm stage, there is a matrix computing step and an iterative LSM (Least Squares Minimization) process presented in Sec. 3.2.1. Finally, the postprocessing stage performs inverse FFT (iFFT) and matrix operations to get the final results. The iterative LSM computation is the most time-consuming part in the entire tensor completion process both on CPUs and GPUs. Therefore, it is the key part to be parallelized and accelerated on powerful GPUs.

\subsubsection{Least Squares Minimization}
As shown in Fig. \ref{pic_gpu_algorithm}, LSM (Least Squares Minimization) is the second step of the Tubal-Alt-Min algorithm, which is the most time-consuming part of the entire algorithm. There are many approaches for least squares minimization. QR factorization is one of the most efficient approaches, which is well supported by CUDA.

\textit{Implementation of QR Factorization:}
The least squares problem can be formulated as:
\[
\widetilde{\mathbf{x}} = \mathop{\arg \min}_{\mathbf{x}} \| \mathbf{b} - \mathbf{A} \mathbf{x} \|^2_F,
\]
where $\mathbf{A}$ is a matrix of size $m \times n$, $\mathbf{x}$ is a vector
of size $k \times 1$, and $\mathbf{b}$ is a vector of size $n \times 1$. In the tensor
algorithm, the system we work on is an overdetermined system, where $n >
 k$.
 We perform QR factorization on $\mathbf{A}$ and get $\mathbf{A} = \mathbf{Q}\mathbf{R}$ , where $\mathbf{Q}$ is an
 orthogonal matrix of size $n \times k $ and $\mathbf{R}$ is an upper triangular matrix of size $k \times k$. Taking advantage of the special nature of these matrices we obtain $\mathbf{x}$
as following:
\[
\mathbf{x} = \mathbf{R}^{-1}\mathbf{Q}^{\dagger} \ast \mathbf{b}.
\]


\textit{Compressed Sparse Row (CSR) Format:}
Considering the sparsity of the matrices grouped by $n$ slices, we choose the CSR format
to store the matrices to save memory space. The $m \times n$ sparse matrix $A$ is represented in CSR format using the parameters in Table \ref{tbl_CSR_parameters}.
\begin{table}[t]
    \centering
    \scriptsize
    \caption{CSR parameters.}
    \label{tbl_CSR_parameters}
    \begin{tabular}{|p{3.5cm}|p{2cm}|p{5cm}|}
        \hline
        Name & Type & Description\\
        \hline
        \textbf{nnz} & (integer) & The number of nonzero elements in the
        matrix. \\
        \hline
        \textbf{csrValA} & (pointer) & Points to data array of length
        \textbf{nnz} that holds all nonzero values of $A$ in row-major
        format. \\
        \hline
        \textbf{csrRowPtrA} & (pointer) & Points to the integer array
        of length \textbf{m+1} that holds indices into the array
        \textbf{csrColIndA} and \textbf{csrValA}. The first \textbf{m}
        entries of this array contain the indices of the first nonzero
        element in the $i$th row for $i = i, \dots, m$, while the last
        entry contains \textbf{nnz+csrRowPtrA(0)}. In general,
        \textbf{csrRowPtrA(0)} is $0$ or $1$ for zero- and one-based
        indexing, respectively. \\
        \hline
        \textbf{csrColInA} & (pointer) & Points to the integer array
        of length \textbf{nnz} that contains the column indices of the
        corresponding elements in array \textbf{csrValA}. \\
        \hline
    \end{tabular}
\end{table}
Sparse matrices in CSR format are assumed to be stored in
row-major format. In other words, the index arrays are first
sorted by row indices and then within the same row by column
indices. It is assumed that each pair of row and column indices appears
only once. Take a $4 \times 5$ matrix $\mathbf{A}$ as an example:
\[
\begin{bmatrix}
    1.0 & 4.0 & 0.0 & 0.0 & 0.0 \\
    0.0 & 2.0 & 3.0 & 0.0 & 0.0 \\
    5.0 & 0.0 & 0.0 & 7.0 & 8.0 \\
    0.0 & 0.0 & 9.0 & 0.0 & 6.0
\end{bmatrix}
\]
The corresponding storage in CSR format with zero-based indexing is as following:
\[
csrVal = \begin{bmatrix}
    1.0 & 4.0 & 2.0 & 3.0 & 5.0 & 7.0 & 8.0 & 9.0 & 6.0
\end{bmatrix},
\]
\[
csrRowPtrA = \begin{bmatrix}
    0 & 2 & 4 & 7 & 9
\end{bmatrix},
\]
\[
csrColInA = \begin{bmatrix}
    0 & 1 & 1 & 2 & 0 & 3 & 4 & 2 & 4
\end{bmatrix}.
\]
By utilizing CSR format, tensor data can be stored in GPU device memory with less space, allowing GPUs to handle tensors of larger size.



\subsubsection{Optimizations}
This section summarizes the optimizations implemented in the GPU algorithm, including slice merging, replacing matrix operations, and data transfer and memory accesses.
\textit{Slice Merging:}
For reading convenience, the CPU implementation in Matlab of work \cite{XiaoYang2016Low} was implemented following the mathematical form. For most parts of the tensor completion algorithm, we follow the steps in the CPU implementation and parallelize the code in CUDA. In order to achieve better performance on the GPU, we reorganized the computation of the least squares minimizations iterations, which is the most time-consuming part in tensor completion both on CPUs and GPUs.

For GPU applications, one of the keys to good performance is to keep the GPU multiprocessors as busy as possible. The least squares minimization is applied on a tensor slice by slice. Different slices are independent so that they can be processed on the GPU in parallel. Therefore we merge the solvers of $n$ independent slices into one solver as following:
\[
\begin{bmatrix}
    \mathbf{b}_1 \\
    \mathbf{b}_2 \\
    \vdots \\
    \mathbf{b}_n
   \end{bmatrix}
   =
   \begin{bmatrix}
       \mathbf{A}_1 & \qquad & \qquad & \qquad \\
       \qquad & \mathbf{A}_2 & \qquad & \qquad \\
       \qquad & \qquad & \ddots & \qquad \\
       \qquad & \qquad & \qquad & \mathbf{A}_n
    \end{bmatrix}
\begin{bmatrix}
    \mathbf{x}_1 \\
    \mathbf{x}_2 \\
    \vdots \\
    \mathbf{x}_n
   \end{bmatrix}.
   \]



\textit{Replacing Matrix Operations:}
The estimation of the j-th lateral slice is transformed into the following standard least squares minimization problem:
\[
\widetilde{\mathbf{x}} = \mathop{\arg \min}_{\mathbf{x} \in
\mathbb{R}^{rk \times 1}} \|\mathbf{b}-\mathbf{A}_3\mathbf{A}_1 \mathbf{x} \|^2_F,
\]
where the $\mathbf{A}_3\mathbf{A}_1\mathbf{x}$ is obtained through many matrix
operations. These operations require time for comparing the matrix
obtained and waste considerable computing resources. To improve performance, we use loops to replace these matrix operations, as illustrated in Fig. \ref{pic_replace_matrix}.
\begin{figure}[t]
    \centering
    \includegraphics[width=4in]{matrix_code.pdf}
    \caption{Replacing matrix operations.}
    \label{pic_replace_matrix}
\end{figure}
In the figure, the omega$\_$f and X$\_$f represents $\Omega$ and $\mathcal{X}$ in frequency domain. This method can improve performance both in CPU and GPU algorithms.

\textit{Data Transfer and Memory Accesses:}
The peak theoretical bandwidth between the device memory and the GPU processor is much higher (288 GB/s on the NVIDIA Tesla M40) than the peak theoretical bandwidth between host memory and device memory (32 GB/s on the PCIe x16 Gen3). Therefore, for best overall application performance, it is important to minimize data transfers between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU.

In the GPU algorithm, we implement two techniques to minimize the data transfer. First, we delicately manage the intermediate data. All intermediate data structures are created in device memory, accessed by the GPU processor, and destroyed without being mapped by the host or copied to host memory. Second, we batch many small transfers into one larger transfer and this performs significantly better than making each transfer separately,  even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer.

For better memory access performance, we use page-locked or pinned memory to store the data since they can attain the highest bandwidth between the host and the device.

\subsection{Open-Source Library}
TBD:list the cuTensor3D APIs here.\\
The goal of this work is to provide users a way for GPU-accelerated tensor completion algorithm.  Therefore we encapsulate the GPU algorithm into an open-source library, which exposes simple host-callable C language APIs. The library along with documents are available at:\\ http://www.findai.net/download.html.


\section{Applications}

\subsection{Tensor SVD for Video Compression}
Based on t-SVD our first method for compression, which we call t-SVD compression, basically follows the same idea of truncated SVD but in the \textit{Fourier domain}. For an $n_1 \times n_2 \times n_3$ tensor $\mathcal{M}$, we use $Algorithm$ to get $\widetilde{\mathcal{M}}$, $\widetilde{\mathcal{U}}$, $\widetilde{\mathcal{S}}$ and $\widetilde{\mathcal{V}}^T$. It is known that $\widetilde{\mathcal{S}}$ is a f-diagonal tensor with each frontal slice is a diagonal matrix. So that total number of f-diagonal entries of $\widetilde{\mathcal{S}}$ is $n_0n_3$ where $n_0$ = min$(n_1, n_2)$. We choise an integer $k_1$, $1 \leq k_1 \leq n_0n_3$ and keep the $k_1$ largest f-diagnal entries of $\widetilde{\mathcal{S}}$ then set the rest to be 0. If $\widetilde{\mathcal{S}}(i,i,j)$ is set to be 0, the let the corresponding columns $\widetilde{\mathcal{U}}(:,i,j)$ and $\widetilde{\mathcal{V}}^T(:,i,j)$ also be 0. We theb call the resulting tensors $\widetilde{\mathcal{U}}_{k_1}$, $\widetilde{\mathcal{S}}_{k_1}$ and $\widetilde{\mathcal{V}}^T_{k_1}$. So the approximation is $\mathcal{M}_{k_1}$ = $\mathcal{U}_{k_1} \ast \mathcal{S}_{k_1} \ast \mathcal{V}^T_{k_1}$ where $\mathcal{U}_{k_1}$, $\mathcal{S}_{k_1}$ and $\mathcal{V}^T_{k_1}$ are the inverse Fourier transforms of $\widetilde{\mathcal{U}}_{k_1}$, $\widetilde{\mathcal{S}}_{k_1}$ and $\widetilde{\mathcal{V}}^T_{k_1}$ along the third dimension. The compression ratio rate for this method is 
$$
\mathrm{ratio_{t-SVD}} = \frac{n_1n_2n_3}{k_1(n_1 + n_2 + 1)}
$$
where $1 \leq k_1 \leq n_0n_3$.

\subsection{Low-tubal-rank Tensor Operation}
In this section, we describe the experiment methodology including the hardware and software platform, testing data, testing process, and the comparison metrics.

\begin{table}[t]
  \renewcommand{\arraystretch}{1.3}
  \centering
  \scriptsize
  \caption{Tesla M40 specifications.}
  \begin{tabular}{ll}
    \hline
    \textbf{Device parameters} & \textbf{Value}\\
\hline
    SMs / CUDA processors & 24 / 3072\\
    Core / Memory clock & 1.11 GHz / 3.0 GHz\\
    Memory Bus Width & 384-bit\\
    Warp size & 32 \\
    Max threads per SM & 2048\\
    Max threads per block & 1024\\
    Max dim of a block (x, y, z) & (1024, 1024, 64)\\
    Max dim of a grid (x, y, z) & (2147483647, 65535, 65535)\\
    Maximum Texture Dimension Size (x, y, z) & 1D = (65536)\\
                                           & 2D = (65536, 65536)\\
                                           & 3D = (4096, 4096, 4096)\\
    Total amount of global memory & 24 GBytes\\
    L2 Cache Size & 3 MBytes\\
    Total amount of constant memory & 64 KBytes\\
    Total amount of shared memory per block & 48 KBytes\\
    Total number of registers per block & 65536\\
    Maximum Layered 1D Texture Size, (num) layers & 1D = (16384), 2048 layers\\
    Maximum Layered 2D Texture Size, (num) layers & 2D = (16384, 16384), 2048 layers\\
    Concurrent copy and kernel execution & Yes with 2 copy engines\\
    CUDA capability & 5.2\\
    \hline
  \end{tabular}
  \label{tbl_M40_specifications}
\end{table}
\subsubsection{Hardware and Software Platform}
We use an NVIDIA Tesla M40 GPU card to evaluate performance of the cuTensor library. Table \ref{tbl_M40_specifications} presents the detailed specification of the Tesla M40 GPU. It is the most powerful product in the Tesla series of Maxwell architecture~\cite{harris2014maxwell}. It consists of 3072 CUDA cores, each having a fully pipelined integer Arithmetic Logic Unit (ALU) and a Floating Point Unit (FPU). These 3072 processors are grouped into 24 Maxwell Streaming Multiprocessors (SMMs), each of which contains 128 processors. There are four independent processing units in each SMM and each processing unit has two instruction units. In every clock cycle, each of the two instruction units selects an ready instruction and issues it to a group of CUDA cores or load/store units (LD/ST) or Special Function Units (SFUs). This is called the dual issue. Tesla M40 has a 24GB device memory, which is the largest among all Maxwell GPUs and earlier generation GPUs. The M40 GPU card is installed on a server with 128 GB memory and two Intel Xeon E5-2640 V4 CPUs clocked at 2.4 GHz. Each CPU incorporates 25 MB cache and 10 physical cores supporting maximum 20 threads with hyperthreading.

The server is running Red Hat enterprise linux server release 7.2 (Maipo) with the kernel version 3.10.0. There is a Matlab R2014b installed that executes the CPU implementation in our work \cite{XiaoYang2016Low} as the baseline for comparison with cuTensor library. NVIDIA CUDA 8.0 \cite{cuda8} is used in all experiments of cuTensor execution.

\subsubsection{Testing Data}
In the experiments, we use both synthetic and real datasets to test the algorithms. The synthetic data is generated according to the low-tubal-rank tensor model. It serves as well-controlled inputs for understanding the CPU and GPU implementations of the Tubal-Alt-Min algorithm, which completes a third-order tensor of size $m \times n \times k$ with tubal-rank $r$ from $|\Omega|$ observed elements.

For real dataset, we use a CPark video \cite{CPark} clip of $640 \times 360 \times 10$ ($640 \times 360$ resolution with 10 frames) with tubal-rank 40, sample and recover it with our algorithm.


\subsubsection{Testing Process}
The synthetic and real datasets are processed by the Matlab code on CPUs and the cuTensor library on GPUs, respectively. We repeat each experiment five times and report the average results.



\subsubsection{Comparison Metrics}
We compare the CPU algorithm with the GPU algorithm in three metrics: running time, relative error rate and convergence speed.
\begin{itemize}
  \item \textit{running time:} varying the tensor size and fixing other parameters, we measure the execution time of the Tubal-Alt-Min algorithm stage in Fig. \ref{pic_gpu_algorithm} as the running time. The execution of the initializing stage and the postprocessing stage takes only a few seconds both on CPUs and GPUs, which is trivial compared to thousands of seconds running time. Finally we calculate speedups as the CPU time divided by the GPU time.
  \item \textit{error rate:} we adopt the metric relative square
      error, defined as $ RSE = \| \widehat{\mathcal{T}} - \mathcal{T} \|_F / \|\mathcal{T} \|_F $.
  \item \textit{convergence speed:}we measure decreasing rate of the RSE across iterations by linearly fitting the measured RSEs.
\end{itemize}

\subsection{Tensor Operations Results}
TBD:Put the results of tensor operations here.

\subsection{Tensor Completion Application Results}
\subsubsection{Running Time of Synthetic Data}
\begin{figure}[t]
    \centering
    \includegraphics[width=3.5in]{TensorSizeVSrunTimeNew.pdf}
    \caption{Running time of the GPU algorithm and the CPU algorithm.}
    \label{pic_runningtime}
\end{figure}

\begin{table}[t]
  \renewcommand{\arraystretch}{1.3}
  \centering
  \scriptsize
  \caption{Running time under different tensor size ($n \times n \times 20$).}
  \begin{tabular}{|l|l|l|l|l|l|}
    \hline
    \textbf{\textbf{n}} & \textbf{200}& \textbf{400}& \textbf{600} & \textbf{800} & \textbf{1000}\\
    \hline
    CPU time(S) & 3056 & 29713 & 83574 & 171347 & 401132\\
    \hline
    GPU time(S) & 143 & 635 & 1364 & 2134 & 3684\\
    \hline
    Speedups & 21.25 & 46.79 & 61.27 & 80.29 & 108.88\\
    \hline
  \end{tabular}
  \label{tbl_runtime}
\end{table}

Fig.~\ref{pic_runningtime} shows the running time of the GPU algorithm and the CPU algorithm for tensors of size $n \times n \times 20$ of tubal-rank 5, where $n$ varies from 200 to 1000 at a step of 200. The sampling rate is set to 50\%, and both CPU and GPU algorithms iterate 15 iterations for completion. The detailed time value is listed in Table \ref{tbl_runtime}. When tensor size grows from $200 \times 200 \times 20$ to $1000 \times 1000 \times 20$,  the CPU time increases from 3056 seconds to 401132 seconds. At the same time, the GPU time increases from 143 seconds to 3684 seconds. The LSM iterations are the most time-consuming step in the tensor completion process, and they account for more than 99\% of the running time. When effectively parallelized and accelerated by the GPU, the execution time is significantly reduced.

\begin{figure}[t]
    \centering
    \includegraphics[width=3.5in]{TensorSizeVSspeedupNew.pdf}
    \caption{Speedups of the GPU algorithm over the CPU algorithm.}
    \label{pic_speedup}
\end{figure}

Fig.~\ref{pic_speedup} presents the speedups calculated as $CPU\_time / GPU\_time$. With the tensor size grows from $200 \times 200 \times 20$ to $1000 \times 1000 \times 20$, the speedup increases rapidly from 21.25 to 108.88. Table \ref{tbl_runtime} shows the detailed speedups of all tensor sizes. These significant speedups are acquired because of the massive parallelism as well as the high memory bandwidth of the GPU. GPUs are multiple SIMT (Single Instruction Multiple Threads) architectures. Given 3072 CUDA cores, the M40 GPU can execute tens of thousands of threads on these cores simultaneously, therefore the LSM method proceeds much faster on the GPU than on the CPU. The speedups are smaller for tensors of smaller size such as $200 \times 200 \times 20$, mainly because that the data parallelism is smaller therefore the GPU resources are not fully utilized. Besides, we can see that the speedups keep increasing with the growing of tensor size. However, we are not able to evaluate tensors larger than $1000 \times 1000 \times 20$ because of the limitation of GPU memory capacity.

\subsubsection{Error Rate of Synthetic Data}
\begin{figure}[t]
    \centering
    \includegraphics[width=3.5in]{samplingrate.pdf}
    \caption{Recovery error RSE in log-scale for different sampling rates.}
    \label{pic_samplingrate}
\end{figure}
This experiment compares the error rate of the CPU and GPU algorithms under different sample rate. We use a low-tubal-rank tensor of size $200 \times 200 \times 50$ with tubal-rank $10$ which is generated by a tensor product of two Gaussian tensors of sizes $200 \times 10 \times 50$ and $10 \times 200 \times 50$. Both the CPU algorithm and the GPU algorithm run for 10 iterations and terminate.

As shown in Fig. \ref{pic_samplingrate}, when sample rate varies from 10\% to 90\%, the RSEs after 10 algorithm iterations drop significantly. Under higher sample rate, the sampled data is more close to the original data,  and consequently the error rate is lower after tensor completion. More importantly, the CPU algorithm and the GPU algorithm achieve almost the same RSEs at all sample rate (the two curves in Fig. \ref{pic_samplingrate} overlap), which means that the two algorithms have similar error rate performance in tensor completion.


\subsubsection{Convergence Speed of Synthetic Data}
\begin{figure}[t]
    \centering
    \includegraphics[width=3.5in]{iterations.pdf}
    \caption{Recovery error RSE in log-scale for different iterations.}
    \label{pic_iterations}
\end{figure}
This experiment compares the convergence speed of the two algorithms under different iterations. We use a low-tubal-rank tensor of size $200 \times 200 \times 50$ with tubal-rank $10$ and a fixed sample rate 50\%.

As shown in Fig. \ref{pic_iterations}, when the two algorithms execute 0 to 10 iterations, the RSEs of two algorithms both drop significantly. The RSEs of the two algorithms have small differences for small number of iterations (e.g. 7 or less iterations). For more iterations (e.g. 8 or more iterations), the RSEs of the two algorithm are almost identical. This means that the two algorithms have similar convergence speed in tensor completion.


\subsubsection{Recover Quality and Running Time of Real Data}
In order to verify how the cuTensor library works with real data, we perform experiments with a real CPark video \cite{CPark} that was used in \cite{XiaoYang2016Low}. The resolution of the video is $640 \times 360 \times 36,000$ and the 8, 001th to the 8, 010th frames are used, which takes up 141KB storage space. When we randomly sample 50\% of the video, the sampled video file size is roughly halved. The sampled video clip of 10 frames constitute a tensor of $640 \times 360 \times 10$ with tubal-rank 40. Finally we apply both the CPU and GPU algorithms and execute for 10 iterations to complete this tensor.
\begin{figure}[t]
  \centering
  \begin{minipage}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{Original_CP.PNG}
    \caption{The original video.}
  \end{minipage}
  \begin{minipage}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{Sampled_CP.PNG}
    \caption{The sampled video.}
  \end{minipage}
  \caption{The original and the sampled video.}
  \label{pic_original}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{minipage}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{CPU_CP.PNG}
    \caption{The recovered video on the CPU.}
  \end{minipage}
  \begin{minipage}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth]{GPU_CP.PNG}
    \caption{The recovered video on the GPU.}
  \end{minipage}
  \caption{The recovered video using the CPU and GPU algorithms.}
  \label{pic_recovery}
\end{figure}


Figs. \ref{pic_original} (a) and (b) show the original video and the sampled video, respectively. In the sampled video, there are lots of noises and some details are missing at the 50\% sample rate. Figs. \ref{pic_recovery} (a) and (b) present the completed/recovered video using the CPU algorithm and the GPU algorithm, respectively. First, we can see that the videos recovered by the CPU algorithm and the GPU algorithm have similar visual effects. Second, by comparing the recovered video with the sampled video, we can see that not only the noises are significantly reduced, but also many details are recovered. Actually, the tubal-rank of this video data is not low, but our algorithms still work well. So this experiment proves the robust of both the CPU algorithm and the GPU algorithms. The time to recover the video is 53760 seconds and 1154 seconds on the CPU and GPU respectively using our algorithm. Therefore, the GPU algorithm achieves a 46.59x speedup versus the CPU algorithm.

\section{Discussion and Extension}
TBD:put some in-depth discussion here.


\section{Conclusions and Future Work}
In this work, we present a cuTensor open-source library for efficient low-tubal-rank tensor completion on GPUs. The experiment evaluations show that the proposed GPU algorithm completes low-tubal-rank tensor effectively. Comparing with the counterpart on CPU, the GPU algorithm achieves similar error rate and convergence speed, and with much faster speed. For synthetic data, the GPU algorithm achieves up to 108.88x speedup versus the CPU algorithm for large-scale tensors. For real video data of medium tensor size, the GPU algorithm recovers the sampled video with good visual effects and achieves 46.59x speedup over the CPU algorithm. The cuTensor library for GPUs is useful for efficient completion of incomplete tensor data in the IoT world.

\section*{Acknowledgment}
The authors would like to thank anonymous reviewers for their fruitful feedback and comments that have helped them improve the quality of this work. This research is supported by Natural Science Foundation of Shanghai under grant No. 17ZR1409800 and Shanghai Innovation Action Plan Project under grant No.16511101200.


%\section*{References}

\bibliographystyle{ACM-Reference-Format}
\bibliography{mybibfile}


\end{document}
